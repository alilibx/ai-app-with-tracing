// ============================================
// LLM-as-a-Judge Evaluation Queries
// ============================================
// These queries help you analyze the quality of AI responses
// using model-as-a-judge evaluation scores.

// Query 1: View All Evaluation Scores with Reasoning
// Shows all evaluation metrics with their scores and reasoning
dependencies
| where timestamp > ago(7d)
| where name startswith "gen_ai.evaluation."
| extend
    evaluator = tostring(customDimensions["gen_ai.evaluator.name"]),
    score = todouble(customDimensions["gen_ai.evaluation.score"]),
    response_id = tostring(customDimensions["gen_ai.response.id"]),
    reasoning = tostring(customDimensions["evaluation.reasoning"])
| project timestamp, response_id, evaluator, score, reasoning
| order by timestamp desc, response_id, evaluator;

// Query 2: Average Scores by Evaluation Criterion
// Aggregate scores to see overall quality trends
dependencies
| where timestamp > ago(7d)
| where name startswith "gen_ai.evaluation."
| extend
    evaluator = tostring(customDimensions["gen_ai.evaluator.name"]),
    score = todouble(customDimensions["gen_ai.evaluation.score"])
| summarize
    avg_score = round(avg(score), 3),
    min_score = round(min(score), 3),
    max_score = round(max(score), 3),
    count = count()
    by evaluator
| order by evaluator;

// Query 3: Evaluation Scores Over Time
// Track evaluation trends to spot quality changes
dependencies
| where timestamp > ago(7d)
| where name startswith "gen_ai.evaluation."
| extend
    evaluator = tostring(customDimensions["gen_ai.evaluator.name"]),
    score = todouble(customDimensions["gen_ai.evaluation.score"])
| summarize avg_score = round(avg(score), 3) by bin(timestamp, 1h), evaluator
| render timechart;

// Query 4: Low-Scoring Responses (Quality Issues)
// Identify responses that need improvement
dependencies
| where timestamp > ago(7d)
| where name startswith "gen_ai.evaluation."
| extend
    evaluator = tostring(customDimensions["gen_ai.evaluator.name"]),
    score = todouble(customDimensions["gen_ai.evaluation.score"]),
    response_id = tostring(customDimensions["gen_ai.response.id"]),
    reasoning = tostring(customDimensions["evaluation.reasoning"])
| where score < 0.7  // Flagging scores below 0.7
| project timestamp, response_id, evaluator, score, reasoning
| order by score asc, timestamp desc;

// Query 5: Join Evaluations with Original Request
// See user query, response, and all evaluation scores together
let evaluations = dependencies
| where timestamp > ago(7d)
| where name startswith "gen_ai.evaluation."
| extend
    evaluator = tostring(customDimensions["gen_ai.evaluator.name"]),
    score = todouble(customDimensions["gen_ai.evaluation.score"]),
    response_id = tostring(customDimensions["gen_ai.response.id"]),
    reasoning = tostring(customDimensions["evaluation.reasoning"]),
    operation_id = operation_Id
| summarize scores = make_bag(pack(evaluator, pack("score", score, "reasoning", reasoning))) by response_id, operation_id;
let requests = dependencies
| where timestamp > ago(7d)
| where name == "weather_chat_function"
| extend
    user_message = tostring(customDimensions["user.message"]),
    response_id = tostring(customDimensions["gen_ai.response.id"]),
    operation_id = operation_Id
| project timestamp, operation_id, response_id, user_message;
requests
| join kind=inner evaluations on response_id, operation_id
| project timestamp, response_id, user_message, scores
| order by timestamp desc;

// Query 6: Evaluation Token Usage and Cost
// Track the cost of running LLM-as-a-judge evaluations
dependencies
| where timestamp > ago(7d)
| where name startswith "llm_judge."
| extend
    criterion = tostring(customDimensions["gen_ai.evaluation.criterion"]),
    input_tokens = toint(customDimensions["gen_ai.usage.input_tokens"]),
    output_tokens = toint(customDimensions["gen_ai.usage.output_tokens"])
| summarize
    total_input = sum(input_tokens),
    total_output = sum(output_tokens),
    evaluation_count = count()
    by criterion
| extend
    cost_usd = round((total_input / 1000.0 * 0.03) + (total_output / 1000.0 * 0.06), 4)
| project criterion, evaluation_count, total_input, total_output, cost_usd;

// Query 7: Distribution of Scores (Histogram)
// Visualize score distribution to understand quality patterns
dependencies
| where timestamp > ago(7d)
| where name startswith "gen_ai.evaluation."
| extend
    evaluator = tostring(customDimensions["gen_ai.evaluator.name"]),
    score = todouble(customDimensions["gen_ai.evaluation.score"])
| summarize count() by evaluator, score_bucket = round(score, 1)
| order by evaluator, score_bucket;

// Query 8: Correlation Between Evaluation Criteria
// See how different evaluation scores relate to each other
let pivot_scores = dependencies
| where timestamp > ago(7d)
| where name startswith "gen_ai.evaluation."
| extend
    evaluator = tostring(customDimensions["gen_ai.evaluator.name"]),
    score = todouble(customDimensions["gen_ai.evaluation.score"]),
    response_id = tostring(customDimensions["gen_ai.response.id"])
| summarize scores = make_bag(pack(evaluator, score)) by response_id;
pivot_scores
| project response_id,
    relevance = todouble(scores.relevance),
    coherence = todouble(scores.coherence),
    groundedness = todouble(scores.groundedness),
    helpfulness = todouble(scores.helpfulness)
| where isnotnull(relevance) and isnotnull(coherence) and isnotnull(groundedness) and isnotnull(helpfulness);

// Query 9: Failed Evaluations
// Identify when evaluation process encounters errors
dependencies
| where timestamp > ago(7d)
| where name startswith "llm_judge." or name startswith "gen_ai.evaluation."
| where success == false or isnotnull(customDimensions["error.message"])
| extend
    criterion = tostring(customDimensions["gen_ai.evaluation.criterion"]),
    error = tostring(customDimensions["error.message"])
| project timestamp, name, criterion, error, operation_Id
| order by timestamp desc;

// Query 10: Complete Evaluation Report
// Comprehensive view with user query, response, and all scores
let main_requests = dependencies
| where timestamp > ago(24h)
| where name == "weather_chat_function"
| extend
    user_message = tostring(customDimensions["user.message"]),
    response_id = tostring(customDimensions["gen_ai.response.id"]),
    duration_ms = duration / 10000.0
| project timestamp, operation_Id, response_id, user_message, duration_ms, success;
let eval_scores = dependencies
| where timestamp > ago(24h)
| where name startswith "gen_ai.evaluation."
| extend
    evaluator = tostring(customDimensions["gen_ai.evaluator.name"]),
    score = todouble(customDimensions["gen_ai.evaluation.score"]),
    reasoning = tostring(customDimensions["evaluation.reasoning"]),
    response_id = tostring(customDimensions["gen_ai.response.id"])
| summarize
    relevance = maxif(score, evaluator == "relevance"),
    coherence = maxif(score, evaluator == "coherence"),
    groundedness = maxif(score, evaluator == "groundedness"),
    helpfulness = maxif(score, evaluator == "helpfulness"),
    avg_score = round(avg(score), 3)
    by response_id;
main_requests
| join kind=leftouter eval_scores on response_id
| project timestamp, response_id, user_message,
    relevance, coherence, groundedness, helpfulness, avg_score,
    duration_ms, success
| order by timestamp desc;
